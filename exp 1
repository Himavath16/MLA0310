import numpy as np
import random

GRID_SIZE = 5
START_STATE = (0, 0)
MAX_STEPS = 50

rewards = np.zeros((GRID_SIZE, GRID_SIZE))


dirt_cells = [(0,2), (1,3), (3,0), (4,4)]
for d in dirt_cells:
    rewards[d] = 1

obstacles = [(1,1), (2,3), (4,2)]
for o in obstacles:
    rewards[o] = -1


actions = {
    "U": (-1, 0),
    "D": (1, 0),
    "L": (0, -1),
    "R": (0, 1)
}



def step(state, action):
    x, y = state
    dx, dy = actions[action]
    nx, ny = x + dx, y + dy

    if nx < 0 or nx >= GRID_SIZE or ny < 0 or ny >= GRID_SIZE:
        return state, 0

    return (nx, ny), rewards[nx, ny]

def random_policy(state):
    return random.choice(list(actions.keys()))
def greedy_policy(state):
    best_action = None
    best_reward = -float("inf")

    for a in actions:
        _, r = step(state, a)
        if r > best_reward:
            best_reward = r
            best_action = a

    return best_action
def sweep_policy(state):
    x, y = state
    if y < GRID_SIZE - 1:
        return "R"
    elif x < GRID_SIZE - 1:
        return "D"
    else:
        return random.choice(list(actions.keys()))

def simulate(policy, policy_name):
    state = START_STATE
    total_reward = 0



    for step_no in range(MAX_STEPS):
        action = policy(state)
        next_state, reward = step(state, action)

        print(f"Step {step_no:02d} | {state} -> {action} -> {next_state} | Reward: {reward}")

        state = next_state
        total_reward += reward

    print("Total Reward Collected:", total_reward)

simulate(random_policy, "Random Policy")
simulate(greedy_policy, "Greedy Policy")
simulate(sweep_policy, "Sweep Policy")
